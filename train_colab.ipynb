{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingredient Recognition Training - Google Colab\n",
        "\n",
        "This notebook trains ResNet-50 or SE-ResNet-50 models for ingredient recognition using HuggingFace datasets.\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better recommended)\n",
        "2. **Clone Repository**: Run Cell 1 to clone from GitHub\n",
        "3. **Run all cells** sequentially\n",
        "4. Training will use streaming HuggingFace datasets (memory efficient)\n",
        "\n",
        "## Features\n",
        "- ‚úÖ GPU acceleration\n",
        "- ‚úÖ Git-based setup (clone from GitHub)\n",
        "- ‚úÖ HuggingFace dataset streaming (no download needed)\n",
        "- ‚úÖ Wandb integration (automatic API key from config)\n",
        "- ‚úÖ Modular trainer structure\n",
        "- ‚úÖ Checkpoint saving\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone Repository from GitHub\n",
        "\n",
        "Update the repository URL below with your GitHub repository URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "‚úì Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# UPDATE THIS: Your GitHub repository URL\n",
        "# ============================================\n",
        "REPO_URL = \"https://github.com/yourusername/your-repo.git\"  # UPDATE THIS!\n",
        "\n",
        "import os\n",
        "\n",
        "# Extract repo name from URL\n",
        "repo_name = REPO_URL.split('/')[-1].replace('.git', '')\n",
        "\n",
        "# Clone repository\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"üì¶ Cloning repository from {REPO_URL}...\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"‚úì Repository cloned\")\n",
        "else:\n",
        "    print(f\"‚úì Repository '{repo_name}' already exists\")\n",
        "\n",
        "# Change to project directory\n",
        "%cd {repo_name}\n",
        "print(f\"‚úì Changed to directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify project structure\n",
        "required_dirs = ['models', 'trainer', 'configs']\n",
        "print(\"\\nüìÅ Verifying project structure:\")\n",
        "for dir_name in required_dirs:\n",
        "    if os.path.exists(dir_name):\n",
        "        print(f\"  ‚úì {dir_name}/ found\")\n",
        "    else:\n",
        "        print(f\"  ‚úó {dir_name}/ missing\")\n",
        "        print(f\"     Make sure your repository contains the {dir_name}/ folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "## 2. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "\n",
            "Contents of /content:\n",
            "==================================================\n",
            "üìÅ Directories:\n",
            "  - .config/\n",
            "  - sample_data/\n",
            "\n",
            "üìÑ Files (showing first 10):\n",
            "==================================================\n",
            "\n",
            "üì¶ Required folders:\n",
            "  trainer/ ‚úó MISSING\n",
            "  models/  ‚úó MISSING\n",
            "\n",
            "==================================================\n",
            "‚ö†Ô∏è  REQUIRED FOLDERS MISSING\n",
            "==================================================\n",
            "\n",
            "Please sync/upload your project files:\n",
            "\n",
            "1. VS Code Colab Extension:\n",
            "   - Make sure you opened this notebook from VS Code\n",
            "   - Files should auto-sync from your local directory\n",
            "   - Check that trainer/ and models/ exist locally\n",
            "\n",
            "2. Manual Upload:\n",
            "   - Use Colab's file browser (folder icon on left)\n",
            "   - Upload trainer/ and models/ folders\n",
            "\n",
            "3. Google Drive:\n",
            "   - Upload project to Drive\n",
            "   - Mount Drive in next cell\n",
            "   - Copy files from Drive\n",
            "\n",
            "4. Git:\n",
            "   - Push project to GitHub\n",
            "   - Clone in Colab: !git clone <your-repo-url>\n",
            "\n",
            "After syncing files, re-run this cell to verify.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install datasets transformers pillow pyyaml wandb scikit-learn matplotlib seaborn tqdm psutil\n",
        "\n",
        "print(\"‚úì Dependencies installed\")\n",
        "\n",
        "# Check what's in the current directory\n",
        "import sys\n",
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "print(f\"Current working directory: {current_dir}\")\n",
        "print(f\"\\nContents of {current_dir}:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List directories and files\n",
        "items = os.listdir(current_dir)\n",
        "dirs = [d for d in items if os.path.isdir(d)]\n",
        "files = [f for f in items if os.path.isfile(f)]\n",
        "\n",
        "print(\"üìÅ Directories:\")\n",
        "for d in sorted(dirs):\n",
        "    print(f\"  - {d}/\")\n",
        "\n",
        "print(f\"\\nüìÑ Files (showing first 10):\")\n",
        "for f in sorted(files)[:10]:\n",
        "    print(f\"  - {f}\")\n",
        "if len(files) > 10:\n",
        "    print(f\"  ... and {len(files) - 10} more files\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check if trainer and models folders exist\n",
        "trainer_exists = os.path.exists('trainer') and os.path.isdir('trainer')\n",
        "models_exists = os.path.exists('models') and os.path.isdir('models')\n",
        "\n",
        "print(f\"\\nüì¶ Required folders:\")\n",
        "print(f\"  trainer/ {'‚úì EXISTS' if trainer_exists else '‚úó MISSING'}\")\n",
        "print(f\"  models/  {'‚úì EXISTS' if models_exists else '‚úó MISSING'}\")\n",
        "\n",
        "# Setup Python path\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "    print(f\"\\n‚úì Added {current_dir} to Python path\")\n",
        "\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.append('/content')\n",
        "    print(\"‚úì Added /content to Python path\")\n",
        "\n",
        "# If folders don't exist, provide instructions\n",
        "if not trainer_exists or not models_exists:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚ö†Ô∏è  REQUIRED FOLDERS MISSING\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nPlease sync/upload your project files:\")\n",
        "    print(\"\\n1. VS Code Colab Extension:\")\n",
        "    print(\"   - Make sure you opened this notebook from VS Code\")\n",
        "    print(\"   - Files should auto-sync from your local directory\")\n",
        "    print(\"   - Check that trainer/ and models/ exist locally\")\n",
        "    print(\"\\n2. Manual Upload:\")\n",
        "    print(\"   - Use Colab's file browser (folder icon on left)\")\n",
        "    print(\"   - Upload trainer/ and models/ folders\")\n",
        "    print(\"\\n3. Google Drive:\")\n",
        "    print(\"   - Upload project to Drive\")\n",
        "    print(\"   - Mount Drive in next cell\")\n",
        "    print(\"   - Copy files from Drive\")\n",
        "    print(\"\\n4. Git:\")\n",
        "    print(\"   - Push project to GitHub\")\n",
        "    print(\"   - Clone in Colab: !git clone <your-repo-url>\")\n",
        "    print(\"\\nAfter syncing files, re-run this cell to verify.\")\n",
        "else:\n",
        "    print(\"\\n‚úì Required folders found! Proceeding to import test...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Please sync trainer/ and models/ folders first!\n",
            "Run the previous cell to check folder status.\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU not available. Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sync Project Files\n",
        "\n",
        "**IMPORTANT**: You need to sync `trainer/` and `models/` folders before proceeding.\n",
        "\n",
        "**Choose one method:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Python path and test imports\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to Python path\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "    print(f\"‚úì Added {current_dir} to Python path\")\n",
        "\n",
        "# Test imports\n",
        "print(\"=\"*50)\n",
        "print(\"Testing Imports\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    from trainer.hf_dataset import HuggingFaceStreamDataset, get_hf_data_loaders\n",
        "    print(\"‚úì trainer.hf_dataset imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Failed to import trainer.hf_dataset: {e}\")\n",
        "\n",
        "try:\n",
        "    from trainer.config import load_config\n",
        "    print(\"‚úì trainer.config imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Failed to import trainer.config: {e}\")\n",
        "\n",
        "try:\n",
        "    from trainer.metrics import calculate_metrics\n",
        "    print(\"‚úì trainer.metrics imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Failed to import trainer.metrics: {e}\")\n",
        "\n",
        "try:\n",
        "    from trainer.validation import validate\n",
        "    print(\"‚úì trainer.validation imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Failed to import trainer.validation: {e}\")\n",
        "\n",
        "try:\n",
        "    from models import create_resnet50, create_se_resnet50\n",
        "    print(\"‚úì models imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Failed to import models: {e}\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create checkpoints directory\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "print(\"\\n‚úì Checkpoints directory created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuration Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your config file path here\n",
        "CONFIG_PATH = 'configs/resnet50_config.yaml'  # Change this to your config file\n",
        "\n",
        "# Verify config exists\n",
        "import os\n",
        "if os.path.exists(CONFIG_PATH):\n",
        "    print(f\"‚úì Config file found: {CONFIG_PATH}\")\n",
        "    # Display config preview\n",
        "    with open(CONFIG_PATH, 'r') as f:\n",
        "        lines = f.readlines()[:15]  # Show first 15 lines\n",
        "        print(\"\\nConfig file preview:\")\n",
        "        print(\"=\"*50)\n",
        "        print(''.join(lines))\n",
        "        if len(lines) == 15:\n",
        "            print(\"... (truncated)\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Config file not found: {CONFIG_PATH}\")\n",
        "    print(\"Please check the path or update CONFIG_PATH\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "# Import the trainer module\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to path (works for both Colab and git clone)\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "# Now we can import from trainer\n",
        "from trainer.config import load_config\n",
        "\n",
        "# Load configuration\n",
        "print(f\"Loading configuration from: {CONFIG_PATH}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "cfg = load_config(CONFIG_PATH)\n",
        "\n",
        "# Print configuration summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Configuration Summary\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model: {cfg['model']}\")\n",
        "print(f\"Dataset: {cfg.get('dataset_name', 'N/A')}\")\n",
        "print(f\"Epochs: {cfg['epochs']}\")\n",
        "print(f\"Batch size: {cfg['batch_size']}\")\n",
        "print(f\"Learning rate: {cfg['lr']}\")\n",
        "print(f\"Optimizer: {cfg['optimizer']}\")\n",
        "print(f\"Scheduler: {cfg['scheduler'].get('type', 'StepLR')}\")\n",
        "print(f\"Wandb: {'Enabled' if cfg['use_wandb'] else 'Disabled'}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Start Training\n",
        "\n",
        "The training will run using GPU acceleration. You can monitor progress in wandb if enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the trainer module\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to path (works for both Colab and VS Code sync)\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.append(current_dir)\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.append('/content')\n",
        "\n",
        "# Now we can import from trainer\n",
        "from trainer.config import load_config\n",
        "\n",
        "# Load configuration\n",
        "print(f\"Loading configuration from: {CONFIG_PATH}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "cfg = load_config(CONFIG_PATH)\n",
        "\n",
        "# Print configuration summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Configuration Summary\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model: {cfg['model']}\")\n",
        "print(f\"Dataset: {cfg.get('dataset_name', 'N/A')}\")\n",
        "print(f\"Epochs: {cfg['epochs']}\")\n",
        "print(f\"Batch size: {cfg['batch_size']}\")\n",
        "print(f\"Learning rate: {cfg['lr']}\")\n",
        "print(f\"Optimizer: {cfg['optimizer']}\")\n",
        "print(f\"Scheduler: {cfg['scheduler'].get('type', 'StepLR')}\")\n",
        "print(f\"Wandb: {'Enabled' if cfg['use_wandb'] else 'Disabled'}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Start Training\n",
        "\n",
        "The training will run using GPU acceleration. You can monitor progress in wandb if enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training using the trainer module\n",
        "# This uses the same code as your local training script\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure we're in the right directory and paths are set\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.append('/content')\n",
        "\n",
        "# Set up sys.argv to simulate command line call\n",
        "original_argv = sys.argv.copy()\n",
        "sys.argv = ['train.py', CONFIG_PATH]\n",
        "\n",
        "try:\n",
        "    # Import and run main function\n",
        "    from trainer.train import main\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(\"Starting Training\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Using GPU: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Run training\n",
        "    main()\n",
        "finally:\n",
        "    # Restore original argv\n",
        "    sys.argv = original_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Download Results (Optional)\n",
        "\n",
        "After training completes, download checkpoints and results to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download checkpoints and results\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Download best model\n",
        "if os.path.exists('checkpoints'):\n",
        "    checkpoint_files = [f for f in os.listdir('checkpoints') if f.endswith('.pth')]\n",
        "    results_files = [f for f in os.listdir('checkpoints') if f.endswith('.json')]\n",
        "    \n",
        "    print(\"Available files to download:\")\n",
        "    for f in checkpoint_files + results_files:\n",
        "        print(f\"  - checkpoints/{f}\")\n",
        "    \n",
        "    # Download all checkpoints\n",
        "    for f in checkpoint_files + results_files:\n",
        "        files.download(f'checkpoints/{f}')\n",
        "        print(f\"‚úì Downloaded: {f}\")\n",
        "else:\n",
        "    print(\"No checkpoints directory found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create __init__.py for models package if it doesn't exist\n",
        "if not os.path.exists('models/__init__.py'):\n",
        "    with open('models/__init__.py', 'w') as f:\n",
        "        f.write('''\n",
        "from .resnet50 import ResNet50, create_resnet50\n",
        "from .se_resnet50 import SEResNet50, create_se_resnet50\n",
        "\n",
        "__all__ = [\n",
        "    'ResNet50',\n",
        "    'create_resnet50',\n",
        "    'SEResNet50',\n",
        "    'create_se_resnet50',\n",
        "]\n",
        "''')\n",
        "\n",
        "# Import model classes\n",
        "from models import create_resnet50, create_se_resnet50\n",
        "\n",
        "# Import other dependencies\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "print(\"‚úì All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize Wandb (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_wandb = cfg.get('use_wandb', False)\n",
        "\n",
        "if use_wandb:\n",
        "    try:\n",
        "        import wandb\n",
        "        \n",
        "        # Login to wandb (first time only - uncomment to login)\n",
        "        # wandb.login()  # Run this once, then comment it out\n",
        "        \n",
        "        # Generate run name if not provided\n",
        "        run_name = cfg.get('wandb_run_name')\n",
        "        if run_name is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            run_name = f\"{cfg['model']}_{timestamp}\"\n",
        "        \n",
        "        wandb.init(\n",
        "            project=cfg.get('wandb_project', 'ingredient-recognition'),\n",
        "            entity=cfg.get('wandb_entity'),\n",
        "            name=run_name,\n",
        "            tags=cfg.get('wandb_tags', []),\n",
        "            config={\n",
        "                'model': cfg['model'],\n",
        "                'epochs': cfg['epochs'],\n",
        "                'batch_size': cfg['batch_size'],\n",
        "                'learning_rate': cfg['lr'],\n",
        "                'weight_decay': cfg['weight_decay'],\n",
        "                'image_size': cfg['image_size'],\n",
        "                'num_workers': cfg['num_workers'],\n",
        "                'se_reduction': cfg.get('se_reduction') if cfg['model'] == 'se_resnet50' else None,\n",
        "                'device': str(device),\n",
        "            }\n",
        "        )\n",
        "        print(f\"‚úì Wandb initialized: {wandb.run.url}\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  wandb not installed. Continuing without wandb.\")\n",
        "        use_wandb = False\n",
        "else:\n",
        "    print(\"Wandb disabled in config\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Setup Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_loaders(data_dir, batch_size=32, num_workers=2, image_size=224):\n",
        "    \"\"\"Create data loaders for training and validation\"\"\"\n",
        "    # Data augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Validation transform (no augmentation)\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Load datasets\n",
        "    train_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=train_transform)\n",
        "    val_dataset = ImageFolder(os.path.join(data_dir, 'val'), transform=val_transform)\n",
        "    \n",
        "    num_classes = len(train_dataset.classes)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader, num_classes, train_dataset.classes\n",
        "\n",
        "# Load data\n",
        "print(\"\\nLoading datasets...\")\n",
        "train_loader, val_loader, num_classes, class_names = get_data_loaders(\n",
        "    cfg['data_dir'], cfg['batch_size'], cfg['num_workers'], cfg['image_size']\n",
        ")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "print(f\"\\nFirst 10 classes: {class_names[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Create Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "print(f\"\\nCreating {cfg['model']} model...\")\n",
        "if cfg['model'] == 'resnet50':\n",
        "    model = create_resnet50(num_classes=num_classes, pretrained=cfg.get('pretrained', True))\n",
        "else:\n",
        "    model = create_se_resnet50(num_classes=num_classes, pretrained=cfg.get('pretrained', True), \n",
        "                               reduction=cfg.get('se_reduction', 16))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Log model info to wandb\n",
        "if use_wandb:\n",
        "    wandb.config.update({\n",
        "        'total_parameters': total_params,\n",
        "        'trainable_parameters': trainable_params,\n",
        "        'num_classes': num_classes,\n",
        "        'train_samples': len(train_loader.dataset),\n",
        "        'val_samples': len(val_loader.dataset),\n",
        "    })\n",
        "    wandb.watch(model, log='all', log_freq=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Setup Optimizer and Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create optimizer based on config\n",
        "optimizer_type = cfg.get('optimizer', 'Adam').lower()\n",
        "if optimizer_type == 'sgd':\n",
        "    sgd_cfg = cfg.get('sgd', {})\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), \n",
        "        lr=cfg['lr'], \n",
        "        weight_decay=cfg['weight_decay'],\n",
        "        momentum=sgd_cfg.get('momentum', 0.9),\n",
        "        nesterov=sgd_cfg.get('nesterov', False)\n",
        "    )\n",
        "else:  # Default to Adam\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
        "\n",
        "# Create scheduler based on config\n",
        "scheduler_cfg = cfg.get('scheduler', {})\n",
        "scheduler_type = scheduler_cfg.get('type', 'StepLR').lower()\n",
        "if scheduler_type == 'cosineannealinglr':\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=cfg['epochs']\n",
        "    )\n",
        "elif scheduler_type == 'reducelronplateau':\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=scheduler_cfg.get('gamma', 0.1), patience=5\n",
        "    )\n",
        "else:  # Default to StepLR\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, \n",
        "        step_size=scheduler_cfg.get('step_size', 15), \n",
        "        gamma=scheduler_cfg.get('gamma', 0.1)\n",
        "    )\n",
        "\n",
        "print(f\"Optimizer: {optimizer_type}\")\n",
        "print(f\"Scheduler: {scheduler_type}\")\n",
        "print(f\"Initial Learning Rate: {cfg['lr']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch, use_wandb=False):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]')\n",
        "    for batch_idx, (images, labels) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Log batch metrics to wandb\n",
        "        if use_wandb and batch_idx % 10 == 0:\n",
        "            wandb.log({\n",
        "                'train/batch_loss': loss.item(),\n",
        "                'train/batch_acc': 100 * (predicted == labels).sum().item() / labels.size(0),\n",
        "                'train/epoch': epoch + 1,\n",
        "                'train/batch': batch_idx\n",
        "            })\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{running_loss / (batch_idx + 1):.4f}',\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device, use_wandb=False):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Validation')\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{running_loss / (pbar.n + 1):.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "\n",
        "def calculate_metrics(all_preds, all_labels, num_classes):\n",
        "    \"\"\"Calculate precision, recall, and F1-score\"\"\"\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='weighted', zero_division=0\n",
        "    )\n",
        "    \n",
        "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=None, zero_division=0\n",
        "    )\n",
        "    \n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'precision_per_class': precision_per_class.tolist(),\n",
        "        'recall_per_class': recall_per_class.tolist(),\n",
        "        'f1_per_class': f1_per_class.tolist(),\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "print(\"‚úì Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create save directory\n",
        "os.makedirs(cfg['save_dir'], exist_ok=True)\n",
        "\n",
        "# Resume from checkpoint if specified\n",
        "start_epoch = 0\n",
        "best_val_acc = 0.0\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "if cfg.get('resume'):\n",
        "    print(f\"\\nResuming from checkpoint: {cfg['resume']}\")\n",
        "    checkpoint = torch.load(cfg['resume'], map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_val_acc = checkpoint['best_val_acc']\n",
        "    history = checkpoint['history']\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for epoch in range(start_epoch, cfg['epochs']):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, \n",
        "                                       optimizer, device, epoch, use_wandb=use_wandb)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, \n",
        "                                                       criterion, device, use_wandb=use_wandb)\n",
        "    \n",
        "    # Update learning rate\n",
        "    if scheduler_type == 'reducelronplateau':\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "    else:\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Log epoch metrics to wandb\n",
        "    if use_wandb:\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train/epoch_loss': train_loss,\n",
        "            'train/epoch_acc': train_acc,\n",
        "            'val/epoch_loss': val_loss,\n",
        "            'val/epoch_acc': val_acc,\n",
        "            'learning_rate': current_lr,\n",
        "        })\n",
        "    \n",
        "    # Print epoch summary\n",
        "    print(f\"\\nEpoch {epoch+1}/{cfg['epochs']}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'history': history,\n",
        "        'num_classes': num_classes,\n",
        "        'class_names': class_names,\n",
        "        'model_type': cfg['model']\n",
        "    }\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        checkpoint['best_val_acc'] = best_val_acc\n",
        "        best_model_path = os.path.join(cfg['save_dir'], f\"{cfg['model']}_best.pth\")\n",
        "        torch.save(checkpoint, best_model_path)\n",
        "        print(f\"‚úì Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "        \n",
        "        # Log best model to wandb\n",
        "        if use_wandb:\n",
        "            wandb.run.summary['best_val_acc'] = best_val_acc\n",
        "            wandb.run.summary['best_epoch'] = epoch + 1\n",
        "            wandb.save(best_model_path)\n",
        "    \n",
        "    # Save latest checkpoint\n",
        "    latest_model_path = os.path.join(cfg['save_dir'], f\"{cfg['model']}_latest.pth\")\n",
        "    torch.save(checkpoint, latest_model_path)\n",
        "    \n",
        "    # Log checkpoint to wandb\n",
        "    if use_wandb:\n",
        "        wandb.save(latest_model_path)\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation with detailed metrics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Evaluation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_val_loss, final_val_acc, final_preds, final_labels = validate(\n",
        "    model, val_loader, criterion, device\n",
        ")\n",
        "\n",
        "metrics = calculate_metrics(final_preds, final_labels, num_classes)\n",
        "\n",
        "print(f\"\\nFinal Validation Results:\")\n",
        "print(f\"Accuracy: {final_val_acc:.2f}%\")\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "# Log final metrics to wandb\n",
        "if use_wandb:\n",
        "    wandb.run.summary.update({\n",
        "        'final_accuracy': final_val_acc,\n",
        "        'final_precision': metrics['precision'],\n",
        "        'final_recall': metrics['recall'],\n",
        "        'final_f1': metrics['f1'],\n",
        "    })\n",
        "    \n",
        "    # Log confusion matrix\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        import numpy as np\n",
        "        \n",
        "        cm = np.array(metrics['confusion_matrix'])\n",
        "        # Plot top 20 classes for readability\n",
        "        if len(class_names) > 20:\n",
        "            class_counts = cm.sum(axis=1)\n",
        "            top_indices = np.argsort(class_counts)[-20:]\n",
        "            cm_plot = cm[np.ix_(top_indices, top_indices)]\n",
        "            class_names_plot = [class_names[i] for i in top_indices]\n",
        "        else:\n",
        "            cm_plot = cm\n",
        "            class_names_plot = class_names\n",
        "        \n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm_plot, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names_plot, yticklabels=class_names_plot)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        wandb.log({'confusion_matrix': wandb.Image(plt)})\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log confusion matrix to wandb: {e}\")\n",
        "\n",
        "# Save final metrics\n",
        "results = {\n",
        "    'model': cfg['model'],\n",
        "    'num_classes': num_classes,\n",
        "    'final_accuracy': final_val_acc,\n",
        "    'final_precision': metrics['precision'],\n",
        "    'final_recall': metrics['recall'],\n",
        "    'final_f1': metrics['f1'],\n",
        "    'history': history,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'config': cfg\n",
        "}\n",
        "\n",
        "results_path = os.path.join(cfg['save_dir'], f\"{cfg['model']}_results.json\")\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Log results file to wandb\n",
        "if use_wandb:\n",
        "    wandb.save(results_path)\n",
        "    wandb.finish()\n",
        "\n",
        "print(f\"\\n‚úì Results saved to {cfg['save_dir']}\")\n",
        "if use_wandb:\n",
        "    print(f\"‚úì Wandb run: {wandb.run.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Download Results\n",
        "\n",
        "Download checkpoints and results to your local machine:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download checkpoints\n",
        "from google.colab import files\n",
        "\n",
        "# Download best model\n",
        "best_model_path = os.path.join(cfg['save_dir'], f\"{cfg['model']}_best.pth\")\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Downloading {best_model_path}...\")\n",
        "    files.download(best_model_path)\n",
        "\n",
        "# Download results JSON\n",
        "results_path = os.path.join(cfg['save_dir'], f\"{cfg['model']}_results.json\")\n",
        "if os.path.exists(results_path):\n",
        "    print(f\"Downloading {results_path}...\")\n",
        "    files.download(results_path)\n",
        "\n",
        "print(\"‚úì Files downloaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Save to Google Drive (Optional)\n",
        "\n",
        "Save checkpoints to Google Drive for permanent storage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy checkpoints to Drive (uncomment to enable)\n",
        "# !cp -r checkpoints /content/drive/MyDrive/ai_coursework/\n",
        "\n",
        "print(\"To save to Drive, uncomment the line above and update the path\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
